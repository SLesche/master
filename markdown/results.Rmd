# Results
```{r}
source("markdown/analysis/load_important_datasets.R")
source("markdown/analysis/manual_review_results.R")
source("markdown/analysis/estimate_reliability_results.R")
source("markdown/analysis/homogeneity_results.R")
source("markdown/analysis/effect_sizes_results.R")
source("markdown/analysis/cor_manual_results.R")
```

All data wrangling and statistical analyses were using `r r_citations$R`. `r r_citations$pkgs`

## Review process
We reviewed results of the CORR and MINSQ algorithms if their fit was below $r_{st} \le .60$ or if $b_j \le 0.65$ or $b_j \ge 1.7$. For the CORR algorithm, out of `r data_manual_review %>% filter(approach == "corr") %>% pull(n) %>% sum()` ERPs evaluated by the algorithm, we inspected `r data_manual_review %>% filter(approach == "corr", origin != "auto") %>% pull(n) %>% sum()` (`r data_manual_review %>% filter(approach == "corr") %>% pull(freq_reviewed) %>% unique() %>% print_percent()`). Of those ERPs, we rejected `r data_manual_review %>% filter(approach == "corr", origin == "rejected") %>% pull(freq) %>% print_percent()` and accepted `r data_manual_review %>% filter(approach == "corr", origin == "accepted") %>% pull(freq) %>% print_percent()` of the results despite their fit. We manually corrected the decisions in `r data_manual_review %>% filter(approach == "corr", !origin %in% c("auto", "accepted", "rejected")) %>% pull(freq) %>% sum() %>% print_percent()` of cases. Automatically rejecting fits $r_{st} \le .20$ discards `r additional_nas_autoreview_corr %>% pull(freq) %>% print_percent()` of latencies. For the MINSQ algorithm, out of `r data_manual_review %>% filter(approach == "minsq") %>% pull(n) %>% sum()` ERPs evaluated by the algorithm, we inspected `r data_manual_review %>% filter(approach == "minsq", origin != "auto") %>% pull(n) %>% sum()` (`r data_manual_review %>% filter(approach == "minsq") %>% pull(freq_reviewed) %>% unique() %>% print_percent()`). Of those ERPs, we rejected `r data_manual_review %>% filter(approach == "minsq", origin == "rejected") %>% pull(freq) %>% print_percent()` of ERPs and accepted `r data_manual_review %>% filter(approach == "minsq", origin == "accepted") %>% pull(freq) %>% print_percent()` of the results despite their fit. We manually corrected the decisions in `r data_manual_review %>% filter(approach == "minsq", !origin %in% c("auto", "accepted", "rejected")) %>% pull(freq) %>% sum() %>% print_percent()` of cases. Automatically rejecting fits $r_{st} \le .20$ discards `r additional_nas_autoreview_minsq %>% pull(freq) %>% print_percent()` of latencies. Because the MINSQ algorithm may fail to find a valid solution if an amplitude parameter of $a_j \le 0$ fits the signal best, we discarded `r data_autoreview %>% filter(approach == "minsq", review == "none") %>% pull(freq) %>% print_percent` of cases. This did not occur in the CORR algorithm.

When reporting the psychometric properties of the algorithms, we will focus on those values passed through manual inspection. Values that were gained from a pipeline ending with the automatic rejection filter are reported in parenthesis. Properties of uninspected pipelines can be found in the respective tables in the appendix.

## Reliability
An overview of Spearman-Brown corrected split-half correlations $r_{tt}$ split by task, measurement window and filter setting can be found in [TABLE]. Across tasks, measurement windows and filter settings the CORR algorithm had mean split-half correlations of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "corr", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` for manually reviewed latencies ($\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "corr", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)` for automatically reviewed latencies), the MINSQ algorithm $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "minsq", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "minsq", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). Area latency measures presented mean split-half correlations of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "uninformed", type == "autoarea", review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. Peak latency measures had mean split-half correlations of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "uninformed", type == "autopeak", review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. The average split-half correlation for values extracted by an expert ERP researcher was $\overline{r_{tt}} = .92$ for area latency measures and $\overline{r_{tt}} = .93$ for peak latency measures [@sadus2023multiverse].
<!-- Here, I looked at the table 3 in Sadus et al. 2023, averaging values for 8Hz, 16Hz and 32Hz condition -->

## Homogeneity
An overview of a method's mean correlation with other methods $r_h$ split by task, measurement window and filter setting can be found in [TABLE]. Across tasks, measurement windows and filter settings the CORR algorithm had a homogeneity of $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "corr", method1_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "corr", method1_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`), the MINSQ algorithm $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "minsq", method1_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "minsq", method1_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). The homogeneity of area latency measures was $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "uninformed", method1_type == "autoarea", method1_review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`, and $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "uninformed", method1_type == "autopeak", method1_review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)` for peak latency measures.

## Effect size
An overview of the effect size of the age effect detected by a particular method split by task, measurement window and filter setting can be found in [TABLE]. Across tasks, measurement windows and filter settings the CORR algorithm had a mean effect size of $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "corr", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "corr", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`), the MINSQ algorithm $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "minsq", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "minsq", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). Area latency measures presented average effect sizes of $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "uninformed", review == "none", type == "autoarea") %>% pull(mean) %>% apa_num(gt1 = FALSE)`, peak latency measures $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "uninformed", review == "none", type == "autopeak") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. The average effect size for values extracted by an expert ERP researcher was $\overline{\omega^2} = .18$ for area latency measures and $\overline{\omega^2} = .17$ for peak latency measures [@sadus2023multiverse].
<!-- Here, I looked at the table 3 in Sadus et al. 2023, averaging values for 8Hz, 16Hz and 32Hz condition -->

## Correlation with manual rater
An overview of the correlation with latency values extracted by an expert ERP researcher [Sadus 2023] split by task, measurement window and filter setting can be found in [TABLE]. Across tasks, measurement windows and filter settings the CORR algorithm had mean correlations of $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "corr", method2_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "corr", method2_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`) with manually extracted latencies, the MINSQ algorithm $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "minsq", method2_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "minsq", method2_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). Area latency measures had a mean correlation of $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "uninformed", method2_review == "none", method2_type == "autoarea") %>% pull(mean) %>% apa_num(gt1 = FALSE)`, peak latency measures $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "uninformed", method2_review == "none", method2_type == "autopeak") %>% pull(mean) %>% apa_num(gt1 = FALSE)`.

## Influence of researcher degrees of freedom
The repeated measures ANOVA with split-half correlation of a particular method as a dependent variable, the between factor filter (8 Hz vs. 16 Hz vs. 32 Hz) and the within factor measurement window (narrow vs. medium vs. wide) showed no effect of filter, $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "corr", review == "manual") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)`, for the CORR algorithm, no effect, $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "minsq", review == "manual") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)`, for the MINSQ algorithm no effect, $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "uninformed", type == "autoarea") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)`, for area latency measures and no effect, $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "uninformed", type == "autopeak") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)`, for peak latency measures. The measurement windows had an effect of $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "corr", review == "manual") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for the CORR algorithm, $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "minsq", review == "manual") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for the MINSQ algorithm, $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "uninformed", type == "autoarea") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for area latency measures and $\omega^2 =$ `r data_nested_reliability %>% filter(approach == "uninformed", type == "autopeak") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for peak latency measures. Consult [TABLE] for a full overview.

When the correlation with manually extracted latencies was used as a dependent variable, the effect of filter settings for the CORR algorithm were $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "corr", method2_review == "manual") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)`, for the MINSQ algorithm $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "minsq", method2_review == "manual") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)`, $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "uninformed", method2_type == "autoarea") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)` for area latency measures and $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "uninformed", method2_type == "autopeak") %>% pull(omega_filter) %>% apa_num(gt1 = FALSE)` for peak latency measures. The measurement windows had an effect of $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "corr", method2_review == "manual") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for the CORR algorithm, $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "minsq", method2_review == "manual") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for the MINSQ algorithm, $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "uninformed", method2_type == "autoarea") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for area latency measures and $\omega^2 =$ `r data_nested_cormanual %>% filter(method2_approach == "uninformed", method2_type == "autoarea") %>% pull(omega_window) %>% apa_num(gt1 = FALSE)` for peak latency measures. Consult [TABLE] for a full overview.

```{r}
# table_mean_reliability_task_filter
```

