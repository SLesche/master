# Results
```{r}
source("markdown/analysis/load_important_datasets.R")
source("markdown/analysis/manual_review_results.R")
source("markdown/analysis/estimate_reliability_results.R")
source("markdown/analysis/homogeneity_results.R")
source("markdown/analysis/effect_sizes_results.R")
source("markdown/analysis/cor_manual_results.R")
```

All data preprocessing and statistical analyses were conducted using `r r_citations$r`. `r r_citations$pkgs`

## Review process
I reviewed results of the MINSQ and MAXCOR approaches if their fit was below $r_{st} \le .60$ or if $b_j \le 0.65$ or $b_j \ge 1.7$.  For the MINSQ algorithm, out of `r data_manual_review %>% filter(approach == "minsq") %>% pull(n) %>% sum()` ERPs evaluated by the algorithm, I inspected `r data_manual_review %>% filter(approach == "minsq", origin != "auto") %>% pull(n) %>% sum()` (`r data_manual_review %>% filter(approach == "minsq") %>% pull(freq_reviewed) %>% unique() %>% print_percent()`). Of those ERPs, I rejected `r data_manual_review %>% filter(approach == "minsq", origin == "rejected") %>% pull(freq) %>% print_percent()` of ERPs and accepted `r data_manual_review %>% filter(approach == "minsq", origin == "accepted") %>% pull(freq) %>% print_percent()` of the results despite their fit. I manually corrected the decisions in `r data_manual_review %>% filter(approach == "minsq", !origin %in% c("auto", "accepted", "rejected")) %>% pull(freq) %>% sum() %>% print_percent()` of ERPs I reviewed. Automatically rejecting fits with $r_{st} \le .20$ discards `r additional_nas_autoreview_minsq %>% pull(freq) %>% print_percent()` of latencies. Because the MINSQ algorithm may fail to find a valid solution if an amplitude parameter of $a_j \le 0$ fits the signal best, I discarded `r data_autoreview %>% filter(approach == "minsq", review == "none") %>% pull(freq) %>% print_percent` of the `r data_manual_review %>% filter(approach == "minsq") %>% pull(n) %>% sum()` total cases. This did not occur in the MAXCOR algorithm.
For the MAXCOR algorithm, out of `r data_manual_review %>% filter(approach == "corr") %>% pull(n) %>% sum()` ERPs evaluated by the algorithm, I inspected `r data_manual_review %>% filter(approach == "corr", origin != "auto") %>% pull(n) %>% sum()` (`r data_manual_review %>% filter(approach == "corr") %>% pull(freq_reviewed) %>% unique() %>% print_percent()`). Of those ERPs, I rejected `r data_manual_review %>% filter(approach == "corr", origin == "rejected") %>% pull(freq) %>% print_percent()` and accepted `r data_manual_review %>% filter(approach == "corr", origin == "accepted") %>% pull(freq) %>% print_percent()` of the results despite their fit. I manually corrected the decisions in `r data_manual_review %>% filter(approach == "corr", !origin %in% c("auto", "accepted", "rejected")) %>% pull(freq) %>% sum() %>% print_percent()` of ERPs I reviewed. Automatically rejecting fits with $r_{st} \le .20$ discards `r additional_nas_autoreview_corr %>% pull(freq) %>% print_percent()` of latencies.

When reporting the psychometric properties of the algorithm, I will focus on those values passed through manual inspection. Values that were gained from a pipeline ending with the automatic rejection filter are reported in parenthesis. Properties of uninspected pipelines can be found in the respective tables.

## Reliability
I estimated reliability using Spearman-Brown corrected split-half correlations. An overview of reliability $r_{tt}$ split by task, measurement window, and filter setting can be found in Tables \@ref(tab:tab-mean-reliability-flanker) - \@ref(tab:tab-mean-reliability-switching). Across tasks, measurement windows, and filter settings the MAXCOR algorithm had a mean reliability of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "corr", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` for manually reviewed latencies ($\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "corr", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)` for automatically reviewed latencies). The MINSQ algorithm had a mean reliability of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "minsq", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "minsq", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). Area latency measures showed a mean reliability of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "uninformed", type == "autoarea", review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. Peak latency measures had a mean reliability of $\overline{r_{tt}} =$ `r mean_reliability_overall %>% filter(approach == "uninformed", type == "autopeak", review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. The average reliability for values extracted by an expert ERP researcher was $\overline{r_{tt}} = .92$ for area latency measures and $\overline{r_{tt}} = .93$ for peak latency measures [@sadus2023multiverse].


<!-- Here, I looked at the table 3 in Sadus et al. 2023, averaging values for 8Hz, 16Hz and 32Hz condition -->

(ref:tab-mean-reliability-flanker) Reliability of different algorithms - Flanker Task
```{r tab-mean-reliability-flanker, tab.cap=paste("(ref:tab-mean-reliability-flanker)"), tab.pos = "h", out.width="75%"}
knit_print(table_mean_reliability_flanker)
```

(ref:tab-mean-reliability-nback) Reliability of different algorithms - Nback Task

```{r tab-mean-reliability-nback, tab.cap=paste("(ref:tab-mean-reliability-nback)")}
knit_print(table_mean_reliability_nback)
```

(ref:tab-mean-reliability-switching) Reliability of different algorithms - Switching Task

```{r tab-mean-reliability-switching, tab.cap=paste("(ref:tab-mean-reliability-switching)")}
knit_print(table_mean_reliability_switching)
```

## Homogeneity
An overview of a method's mean correlation with other methods $r_h$ split by task, measurement window, and filter setting can be found in Tables \@ref(tab:tab-mean-homogeneity-flanker) - \@ref(tab:tab-mean-homogeneity-switching). Across tasks, measurement windows, and filter settings the MAXCOR algorithm had a mean homogeneity of $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "corr", method1_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "corr", method1_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`), and the MINSQ algorithm a mean homogeneity of $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "minsq", method1_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "minsq", method1_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). The mean homogeneity of area latency measures was $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "uninformed", method1_type == "autoarea", method1_review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. The mean homogeneity of peak latency measures was $\overline{r_{h}} =$ `r mean_homogeneity_overall %>% filter(method1_approach == "uninformed", method1_type == "autopeak", method1_review == "none") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. Homogeneity is larger compared to the other measurement windows when a medium-sized measurement window is employed for both the MINSQ ($\overline{r_{h}} =$ `r mean_homogeneity_full_info %>% filter(approach == "minsq", review == "auto", window == "const") %>% pull(mean) %>% mean()` vs. $\overline{r_{h}} =$ `r mean_homogeneity_full_info %>% filter(approach == "minsq", review == "auto", window != "const") %>% pull(mean) %>% mean()`) and the MAXCOR approach ($\overline{r_{h}} =$ `r mean_homogeneity_full_info %>% filter(approach == "corr", review == "auto", window == "const") %>% pull(mean) %>% mean()` vs. $\overline{r_{h}} =$ `r mean_homogeneity_full_info %>% filter(approach == "corr", review == "auto", window != "const") %>% pull(mean) %>% mean()`).

(ref:tab-mean-homogeneity-flanker) Homogeneity of different algorithms - Flanker Task
```{r tab-mean-homogeneity-flanker, tab.cap=paste("(ref:tab-mean-homogeneity-flanker)")}
knit_print(table_mean_homogeneity_flanker)
```

(ref:tab-mean-homogeneity-nback) Homogeneity of different algorithms - Nback Task
```{r tab-mean-homogeneity-nback, tab.cap=paste("(ref:tab-mean-homogeneity-nback)")}
knit_print(table_mean_homogeneity_nback)
```

(ref:tab-mean-homogeneity-switching) Homogeneity of different algorithms - Switching Task
```{r tab-mean-homogeneity-switching, tab.cap=paste("(ref:tab-mean-homogeneity-switching)")}
knit_print(table_mean_homogeneity_switching)
```

## Effect size
An overview of the effect size of the age effect estimated by a particular method and split by task, measurement window, and filter setting can be found in Tables \@ref(tab:tab-mean-effsize-flanker) - \@ref(tab:tab-mean-effsize-switching). Across tasks, measurement windows, and filter settings, the MAXCOR algorithm had a mean effect size of $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "corr", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "corr", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). The MINSQ algorithm had a mean effect size of $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "minsq", review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "minsq", review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). Area latency measures showed average effect sizes of $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "uninformed", review == "none", type == "autoarea") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. Peak latency measures showed $\overline{\omega^2} =$ `r mean_effsize_overall %>% filter(approach == "uninformed", review == "none", type == "autopeak") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. The average effect size for values extracted by an expert ERP researcher was $\overline{\omega^2} = .18$ for area latency measures and $\overline{\omega^2} = .17$ for peak latency measures [@sadus2023multiverse]. 

In the Flanker task data, the MINSQ algorithm with a 32 Hz low-pass filter and a narrow measurement window yielded the largest effect sizes, while peak latency algorithms yielded the lowest effect sizes. In the Switching task, the MINSQ algorithm combined with a wide measurement window yielded the largest effect size estimates, while area latency and peak latency algorithms showed effect sizes of $\omega^2 = 0$ in some conditions. Similarly, in the Nback task, the MAXCOR algorithm combined with a wide measurement window showed the largest effect sizes while area latency and peak latency algorithms showed effect sizes of $\omega^2 = 0$ in some conditions. 
<!-- Here, I looked at the table 3 in Sadus et al. 2023, averaging values for 8Hz, 16Hz and 32Hz condition -->

(ref:tab-mean-effsize-flanker) Effect size for the age effect - Flanker Task
```{r tab-mean-effsize-flanker, tab.cap=paste("(ref:tab-mean-effsize-flanker)")}
knit_print(table_mean_effsize_flanker)
```

(ref:tab-mean-effsize-nback) Effect size for the age effect - Nback Task
```{r tab-mean-effsize-nback, tab.cap=paste("(ref:tab-mean-effsize-nback)")}
knit_print(table_mean_effsize_nback)
```

(ref:tab-mean-effsize-switching) Effect size for the age effect - Switching Task
```{r tab-mean-effsize-switching, tab.cap=paste("(ref:tab-mean-effsize-switching)")}
knit_print(table_mean_effsize_switching)
```

## Correlation with manual rater
An overview of the intraclass correlation of latencies that were extracted by the algorithm with latency values extracted by an expert ERP researcher [@sadus2023multiverse] split by task, measurement window, and filter settings can be found in Tables \@ref(tab:tab-mean-manualcor-flanker) - \@ref(tab:tab-mean-manualcor-switching). Across tasks, measurement windows, and filter settings, the MAXCOR algorithm had mean correlations of $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "corr", method2_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "corr", method2_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`) with manually extracted latencies. The MINSQ algorithm had mean intraclass correlations of $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "minsq", method2_review == "manual") %>% pull(mean) %>% apa_num(gt1 = FALSE)` ($\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "minsq", method2_review == "auto") %>% pull(mean) %>% apa_num(gt1 = FALSE)`). Area latency measures had a mean intraclass correlation of $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "uninformed", method2_review == "none", method2_type == "autoarea") %>% pull(mean) %>% apa_num(gt1 = FALSE)`, peak latency measures $\overline{r} =$ `r mean_manualcor_overall %>% filter(method2_approach == "uninformed", method2_review == "none", method2_type == "autopeak") %>% pull(mean) %>% apa_num(gt1 = FALSE)`. 

In the Flanker task data, the MINSQ approach combined with a medium measurement window led to intraclass correlations with manually extracted latencies consistently above $0.93$, even in the fully automatic approach. The area latency approach showed intraclass correlations between $0.79$ and $0.93$. A similar pattern emerged for the Nback and Switching tasks. The MINSQ approach combined with a medium measurement window led to the highest intraclass correlations, showing values consistently above $0.8$. Other measures or other measurement windows displayed less consistency, showing intraclass correlations $r \le 0.70$ in some conditions.

(ref:tab-mean-manualcor-flanker) Intraclass correlation with manually extracted latencies - Flanker Task
```{r tab-mean-manualcor-flanker, tab.cap=paste("(ref:tab-mean-manualcor-flanker)")}
knit_print(table_mean_manualcor_flanker)
```

(ref:tab-mean-manualcor-nback) Intraclass correlation with manually extracted latencies - Nback Task
```{r tab-mean-manualcor-nback, tab.cap=paste("(ref:tab-mean-manualcor-nback)")}
knit_print(table_mean_manualcor_nback)
```

(ref:tab-mean-manualcor-switching) Intraclass correlation with manually extracted latencies - Switching Task
```{r tab-mean-manualcor-switching, tab.cap=paste("(ref:tab-mean-manualcor-switching)")}
knit_print(table_mean_manualcor_switching)
```


