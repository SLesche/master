# History of Extraction Methods
## Importance of ERPs
In the dynamic landscape of cognitive neuroscience, the accurate extraction of event-related potentials (ERPs) from electroencephalographic (EEG) data stands as a crucial step in understanding the timing of neural processes underlying cognitive functions. In studies focusing on individual differences, reliably extracting ERP latencies becomes paramount but increasingly difficult due to signals with increased impact of noise. To deal with low signal-to-noise ratios, researchers often  Manual extraction of ERP latencies from EEG recordings is a labor-intensive and time-consuming endeavor, often fraught with subjectivity and variability. This paper introduces a novel algorithm specifically tailored for the automatic extraction of ERP latencies, addressing the pressing need for a standardized and streamlined approach in ERP research and improving on existing approaches such as peak latency or area latency algorithms. We demonstrate that this algorithm enables efficient, objective, reliable and valid extraction of ERP latencies. This can help improve both reproducibility and scalability of ERP studies. This becomes especially helpful in multiverse studies, where the scalability of analytical tools is critical.

## Prior Algorithms
### Peak Latency
Automatically extracting component latencies has long been a goal in ERP research. The earliest version of an algorithm to automatically extract ERP latencies is the _peak latency_ approach [REF]. It involves finding the point in time within a set measurement window that has the largest voltage deflection in the appropriate direction.  This method was developed as it was easy to implement **something about historical ease of use** **Luck, 2014, p. 286 cites Donchin & Heffley, 1978 as historical example**. Even though an algorithm detecting the _peak latency_ inside some measurement window is easy to implement and run, it can be corrupted by several factors.

#### Issues with peak latency

1. Choice of measurement window
Peak latency approaches are blind to the general structure within the ERP signal. They locate the point in time with the largest voltage deflection within the measurement window. The "true" maximum signal may lay just outside of the measurement window, resulting in the algorithm picking an outside edge of the window, even though the signal increases in amplitude just following that time-point.  [REF] have modified the simple _peak latency_ algorithm to only consider maxima inside the measurement window which are also larger than the surrounding data-points. A spike just on the edge of the measurement window  will not meet these criteria. This method is more accurate in finding the _local_ peak latency [REF] but still vulnerable to other issues.

2. Superimposition of other components
Later components may also already influence the amplitude on the outer ranges of the measurement window.  _peak latency_ algorithms can thus fail to pick the _local maximum_ of a component of interest in favor of a peak possibly influenced by later or earlier components. This problem is often referred to as the _superimposition problem_ [@luck2014introduction]. 
Researchers have to carefully choose the measurement window in order to include most of the signal related to the component of interest while simultaneously excluding influences of other components.

3. high frequency noise
The influence of high frequency noise presents another issue for peak latency algorithms. The maximum voltage deflection may not reflect the true point in time at which the process underlying the component reaches its maximum. Rather it may be the result of high frequency noise inducing a spike in the signal, independent of any cognitive process. This is problematic especially in later ERP components, as the broader measurement window most commonly applied will result in an increased likelihood of high frequency noise impacting the maximum voltage deflection [@clayson2013noise].

4. The peak latency is also just not that relevant
Lastly, as @luck2014introduction so aptly states: _There is nothing special about the point at which the voltage reaches a local maximum_. The largest deflection does not inherently relate to any physiological or psychological process and may not even reflect the true maximum of the component of interest. @luck2005ten visually demonstrates how peak latency may be a result of the overlap of multiple components and not related to any single component.

### Fractional Area Latency
_Fractional Area Latency_ approaches hope to remedy some of the problems associated with _peak latency_ algorithms. This technique revolves around the area under the ERP signal in a given measurement window. The goal is finding the point in time that divides that area under the signal into a given fraction to the left and right of it. The time-point halving the area under the signal, for example, is referred to as the _50% area latency_. This approach is much less susceptible to the influence of high frequency noise, as short spikes in the signal do not have a strong impact on the area under the signal. It remains highly dependent on the measurement window [@luck2014introduction]. Choosing a shorter window may result in only a part of the area of the component of interest being captured. A wider measurement window on the other hand might be influenced by surrounding components. _Fractional Area_ measures thus work best for investigating an isolated component [@luck2014introduction], limiting the applicability of algorithms based on this.

### Jackknifing
Another approach towards dealing with noisy subject-level ERPs is to try and mitigate that noise by averaging multiple subject-level ERPs. This technique is referred to as _jackknifing_. It uses the grand average of $N$ subject-level ERPs and generates $N$ sub-grand averages by removing one of the subject-level ERPs. This averaging procedure results in $N$ ERPs with higher signal-to-noise ratios. Both _peak_ and _area_ based measures can then be applied to the jackknifed data to extract latencies. As any two jackknifed signals share $\frac{N-2}{N} * 100$ percent of the subject-level signals that are averaged with each other, each jackknifed sub-average is quite similar to all others. This artificially decreases the error variance, which needs to be corrected for when testing for significant differences between groups [@ulrich2001using]. Because latencies extracted from jackknifed ERPs are based on averaged waveforms, they can not readily be associated with any single subject, preventing this method from generating individual-level latency estimates needed for individual differences research. This problem was addressed by [@smulders2010simplifying] who introduced a transformation able to generate individual-level latency estimates.

## Comparison of Algorithms
@kiesel2008measurement compared a number of extraction methods for a variety of ERP components. They simulated latency differences of the visual and auditory N1, the N2pc and the P3 and frequency-related P3 and tested single-participant approaches and jackknife-approaches combined with peak latency, fractional area latency, relative criteria and baseline deviation methods on their ability to detect these effects. The most widely used technique of single-participant approaches combined with peak latency extraction proved to not be the most efficient method to detect latency effects. This becomes especially true as the signal-to-noise ratio decreases. Overall, jackknifing ERPs and using the relative criterion technique or the fractional area latency technique was shown to be the best approach across the components and datasets they analyzed. 

@kiesel2008measurement were concerned with extracting condition differences in ERP research. While this is important, techniques applied in individual differences research have to extract latencies from subject-level ERPs and produce both reliable and valid estimates. They did not assess psychometric properties of latency estimates. Addressing this gap in research, @sadus2023multiverse investigated the influence of different preprocessing strategies and latency extraction techniques on psychometric qualities of the latency values generated and their ability to detect an age-related effect in P3 latency. They varied the strength of the low-pass filter applied to the data, used both single-participant and jackknife approaches and extracted latencies either automatically or manually, using either a peak latency or area latency approach. Both the size of the effect and the psychometric properties, such as reliability or homogeneity of the latency values, varied between the different analysis strategies. No combination of preprocessing steps and extraction method proved best across all tasks and groups. A total of 7 out of 40 possible pipelines generated consistently desirable reliabilities ($r_{tt} \ge .70$), homogeneities ($r_{h} \ge .50$) and effect sizes ($.03 \le \omega^2 <.80$). All of those seven pipelines used manual extraction methods either based on peak or area latencies. While automated extraction methods would improve both efficiency and objectivity, fully automated approaches failed to generate consistently reliable and valid latency measures [@sadus2023multiverse; @schubert2023robust]. Yet, manual extraction methods are highly time-consuming and impede reproducibility. We hope to show that our algorithm can match the performance of manual extraction while providing a more efficient and objective approach for extracting individual component latency values.

# Our algorithm
## Motivation
The shortcomings of prior algorithms may be due to none of them aiming to replicate human behavior and focus on generating similar decisions rather than similar decision processes. We propose an algorithm that more closely resembles the process expert ERP researches employ.

## template matching
### Introduction
To investigate where in some noisy signal the component of interest is found, most researchers have some mental representation of what the component _should_ look like and where it _should_ generally appear. When visually inspecting ERP signals, their goal is to identify a pattern within the signal that resembles the component of interest in shape, size and location.

Finding a given pattern inside a noisy signal is not a novel task. Algorithms aiming to detect the appearance of a pattern - a _template_ -inside audio-, video- or radio signals have been around for over 50 years [REFS]. And a large amount of research has gone into optimizing these _template matching_ algorithms [REF]. 

No matter the implementation details of a particular template matching algorithm, all aim to answer the question "Does this (shorter) template appear in my (larger) signal?". To achieve this, a researcher needs to specify two things. First, the template they want to search the signal for. And second, the _similarity measure_ which quantifies how well the template fits in a given spot of the signal.

### Similarity measures
Specifying the template is mostly a philosophical question dependent on the specific task and type of signal. Choosing a similarity measure on the other hand is much more methodological. Across a number of papers, several different similarity measures have been proposed. They follow one of two general lines of thought [@brunelli1997template; @goshtasby1984two]. The first type of similarity measure aims to minimizes some value reflecting the distance between template and the signal. The second type aims to maximize some form of correlation between signal and template. [[mahalakshmi2012image]] [REFs; Brunelli1997, Brunelli2009, Goshtasby1984]. We have chosen to implement algorithms based on two of the possible measures, one minimizing the Sum of Squared Differences (MINSQ) and one maximizing the correlation (CORR) between the template and the signal. We wanted to implement both a similarity measure following a traditional distance-minimization approach and a correlational approach in order to gauge the efficacy of these approaches when applied to our field of research.

### Template generation
Finding an appropriate template is more challenging. Depending on the field, the template to search for is easily specified. If you are looking to extract a particular audio-signal from a recording or some specific object in an image you can easily use that object as a template [REF]. The difficulty increases if it is not exactly certain what template you are looking for. Recent research in image processing has attempted to use template matching to process faces, for example [REF]. You cannot just use "the ideal set of eyes" to identify a face. Each person comes with their own set of eyes, different from all others in some quantifiable way. A similar issue accompanies attempts of template matching approaches in ERP research. The variance in ERP signals introduced by the task or the sample of participants hinders a successful implementation of template matching algorithms using only one idealized template over all types of studies. Finding a template that reflects the influence of the task or the sample, equivalent to finding the person who's eyes you are looking for, will significantly improve performance. 

## Prior attempts
There have been some attempts at using an idealized signal structure as a template to identify ocular artifacts in noisy subject-level data [@li2006automatic] or to predict subject behavior on a single trial level [@william2020erp]. However, these approaches were not concerned with estimating the timing of components, but only interested in detection of a specific signal.

@borst2015discovery and @anderson2016discovery developed a machine-learning approach that aims to discover cognitive processing stages on a single trial level. In a first step, their algorithm makes use of multivariate pattern analysis to detect "bumps" in the EEG signal representing the onset of a new cognitive state. They assumed that entry into a new state would be accompanied by a spike in all electrodes similar to a 50 ms half-sine. This 50 ms half-sine then serves as a template with which their algorithm tries to detect those "bumps" in activity. However, their assumptions regarding the template and the location of activity are somewhat crude generalizations made necessary by noisy single trial data. Using template matching to extract component latencies from ERPs requires a more informative template. 

## our solution
A simple approach towards designing a more informative template would be to generate an idealized component structure. Prior knowledge about the shape, size and location about the component of interest could then be made use of. One could draw up "the perfect P3" and attempt to use this as a template. Nonetheless, this would neglect the experiment-specific and task-specific variance in the morphology of ERP components, resulting in a template that does not optimally reflect the data. We addressed this problem by using the grand average as an experiment-specific template of the component of interest that will reflect influences of the task and sample on the morphology of the idealized ERP component.  It has a higher signal-to-noise ratio than any individual subject-level ERP while still being influenced by experiment-specific changes in component morphology. Researchers can (and already often do) use the grand average to gather insight into the time window in which the component of interest occurs [@kiesel2008measurement]. The grand average and the specific window in which the component of interest occurs is used as a template and matched to subject-level ERPs. 

### Differences to other template matching studies
The grand average is the mean of all subject-level ERPs and thus minimizes the sum of squared deviations between each subject-level ERP and itself. Thus, across all subjects, it is the best approximation for each subject-level ERP. However, individual differences in the shape, size and location of the component of interest remain. The goal of our algorithm is to quantify these individual differences. This is similar to facial recognition not only detecting the presence of eyes, but also determining the person the eyes belong to. We do not only aim to detect the presence of a component, but also aim to measure individual differences in shape, size and location of the component. 

To achieve this, we introduce variability into the template reflecting individual differences in the morphology of the component. Determining which transformation of the template best fits a given signal will allow us to investigate individual differences.

## Subject-level transformations as the goal
We introduce variability in the template through two sources, amplitude and latency. Variability in amplitude is controlled by the parameter $a$, variability in latency by the parameter $b$. Amplitude of the template is varied by multiplying the whole template-signal by parameter $a$. Latency is varied by "stretching" or "squishing" the template along the x-axis [See Figure?]. The parameter $b$ controls the strength of this transformation. Possible transformations of the template are then compared to the signal and their similarity is evaluated. Recovering the transformation parameters $[a, b]$ that lead to the best match between template and signal thus allows us to describe individual differences in the latency of a component through the transformation parameter $b_j$ for the participant $j$. In this manner, component latencies can be recovered. Researchers need to specify a time-point of the grand average denoting the latency of the component of interest. The optimal transformation parameters are then applied to the grand average latency to recover the subject-level latency of the component.

## Why our algorithm may perform better
This algorithm aims to address some of the issues faced by other algorithms. It makes use of the entire component structure that is used as a template. This reflects the decision process of expert ERP researchers and enables an intuitive understanding of the decisions made by our algorithm. Because the similarity measures take the whole component structure into account, they are robust to peaks in the signal introduced by high frequency noise. Furthermore, the measurement window set by the researcher only impacts the size and shape of the template. It has no direct connection to the subject-level ERP. The influence of measurement windows on the latencies extracted should thus be lower than in peak or area latency algorithms. Lastly, it is important to note that this is not a machine learning algorithm with a neural net representing some "black box" decision making algorithm. Simplicity and traceability of the decision process was an express goal, allowing more insight into the benefits and drawbacks of our algorithm.

## The present study
In order to compare the quality of our proposed algorithm we will reanalyze the same data analyzed by [@sadus2023multiverse]. We can compare the psychometric properties of our algorithm to those of previously established algorithms, investigate the impact of different preprocessing steps and evaluate the correlation between latencies extract by our algorithm and those extracted manually be an expert ERP researcher. 

In their study, @sadus2023multiverse extracted latencies of the P3 component. The P3, more specifically, the P3b, is a centroparietal positive-going component, peaking around 300 ms after stimulus onset. It is a late higher-order cognitive component [REFS] associated with [Processes]. A number of studies have demonstrated a large effect of age on the latency of the P3 across a number of tasks [REFs]. In a multiverse approach @sadus2023multiverse tested several extraction methods with varying preprocessing steps in their ability to detect this age effect. They used three tasks, each measuring one of the executive functions proposed by @miyake2000unity. To measure the functions _updating_, _shifting_ and _inhibition_, they employed an Nback, a Switching and a Flanker Task, respectively. Studying three different tasks allows us to gain insight into a larger variety of higher-order cognitive processing, improving the generalizability of our findings.

Nonetheless, we will restrict our analysis to extracting P3 latencies. It usually has a broad and isolated structure with comparatively low influence of surrounding components [REF]. This makes the P3 one of the easier components for automated latency extraction approaches. After we can demonstrate proof-of-concept for P3 latency extraction, we will evaluate the algorithms ability to be extended to other ERP components. 

We hope to show that a template matching approach using the grand average as a variable template can successfully extract P3 component latencies. Ideally, use of this algorithm will improve psychometric properties in comparison to prior algorithms, show high correlations with manually extracted data and present an objective and efficient way to extract ERP latencies.
